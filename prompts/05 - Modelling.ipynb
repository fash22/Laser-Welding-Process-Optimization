{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "def get_completion_from_messages(messages, model=\"gpt-3.5-turbo\", temperature=0, max_tokens=1000):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Machine Learning Model Comparison\n",
       "\n",
       "In the previous section on Data Wrangling, it was discovered that the 6 machine parameters to be studied had discrete values with three levels each, and the target variable was a highly imbalanced binary variable.\n",
       "\n",
       "In this section, we will feed the dataset into different machine learning models to find the optimal model that can help us understand the relationship between the input and output variables. To do this, we will split the dataset into a training set and a testing set, with a test size of 20%. The training set will be used for model training and cross-validation, while the testing set will be used to evaluate the model's performance on unseen data.\n",
       "\n",
       "Once the data is split, we will load it into different out-of-the-box machine learning models from the scikit-learn library. At this stage, our goal is to find the optimal model using their default hyperparameters.\n",
       "\n",
       "Finally, we will compare and evaluate the training and testing performance of each model. The objective is to shortlist the top-performing model.\n",
       "\n",
       "## Importing Relevant Libraries\n",
       "\n",
       "The following code contains all the necessary import statements for this section.\n",
       "\n",
       "## Importing the Dataset\n",
       "\n",
       "The code below loads the pre-processed dataset from the EDA section into a pandas DataFrame.\n",
       "\n",
       "## Vertical Data Splitting\n",
       "\n",
       "Once the dataset is loaded, we will split it into a feature matrix `X` and a target vector `y`.\n",
       "\n",
       "## Horizontal Data Splitting\n",
       "\n",
       "After defining the feature matrix and target vector, we will split them horizontally into a training set and a testing set. The code below performs the horizontal split with a test size of 20%, ensuring that both sets contain the same proportion of `0`s and `1`s in their target variable.\n",
       "\n",
       "## Modelling\n",
       "\n",
       "To compare different models, we will use the yellowbrick data visualization library. This library provides out-of-the-box data visualization tools for comparing different machine learning algorithms implemented in scikit-learn.\n",
       "\n",
       "The class definition below provides an abstraction for comparing different machine learning models. Its constructor accepts a list of estimators, an input feature `X`, and an output variable `y`. It has a `show_classification_report` method that returns a data visualization of each model's ROC curve, confusion matrix, and classification report.\n",
       "\n",
       "To generate these reports, the `ClassifierEvaluator` class internally splits the input feature `X` and output variable `y` into training and testing sets. Each classifier will be trained on the training set. To evaluate the model on unseen data, the `score` method of yellowbrick visualizers will be called using the testing set.\n",
       "\n",
       "## Training the Model\n",
       "\n",
       "The code below will train 5 machine learning models using the preprocessed dataset from the Data Wrangling section.\n",
       "\n",
       "## Classification Report\n",
       "\n",
       "The following series of plots show the ROC curve, confusion matrix, and classification report of each model. This visualization can be used to compare the performance of each model.\n",
       "\n",
       "In conclusion, we have successfully trained and evaluated multiple machine learning models on our dataset. The next step would be to analyze the results and select the top-performing model based on the evaluation metrics. We can then proceed with further fine-tuning and optimization of the chosen model to improve its performance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instructions = \"\"\"\n",
    "Revise the title and the content:\n",
    "\n",
    "\n",
    "# Modelling\n",
    "On the [Data Wrangling](exploring-data.qmd) section, it was found that the 6-machine paramaters to be studied were discrete values with three levels each, and the target variable was a highly imbalanced binary variable.\n",
    "\n",
    "In this section, the dataset will be fed into different machine learning models to find an optimal model that can help us understand the mapping between the input and output variables. To do this, the dataset should be splitted into two parts namely, the training set, and the testing set with test size of 20%. The training set will be used during model training, and cross validation while the testing set will be used to evaluate the model performance on unseen data.\n",
    "\n",
    "Once the data was splitted, it will be loaded to different out-of-the-box machine learning models from scikit-learn library.\n",
    "At this stage, the only goal is to find the optimal model using their default hyperparameters.\n",
    "\n",
    "Finally, each of the model's training and testing performance will be compared and evaluated. The goal is to shortlist the top performing model.\n",
    "\n",
    "## Importing Relevant Libraries\n",
    "The following code below contains all of the import statements for this section.\n",
    "\n",
    "## Importing the Dataset\n",
    "The following code below loads the dataset pre-processed from [EDA](exploring-data.qmd) section into pandas `DataFrame`.\n",
    "\n",
    "## Vertical Data Splitting\n",
    "Once loaded, the dataset will be splitted into feature matrix `X`, and target vector `y`.\n",
    "\n",
    "## Horizontal Data Splitting\n",
    "Once the feature matrix and target vector was defined, they will be splitted horizontally into training set and testing set.\n",
    "The code below will perform the horizontal split with test size of 20% ensuring that both sets contains the same proportion of `0`s and `1`s on their target variable.\n",
    "\n",
    "## Modelling\n",
    "To compare different models, the yellowbrick data visualization library was used. This library contains out-of-the-box data visualization tools for comparing different machine learning algorithms implemented in scikit-learn.\n",
    "\n",
    "The class definition below provides an abstraction in comparing different machine learning models.\n",
    "Its constructor accepts a list of estimators, an input feature X, and output variable y. It has a show_classification_report method that returns a data visualization of each of the model's ROC curve, confusion matrix, and classification report.\n",
    "\n",
    "To generate these reports, the ClassifierEvaluator class internally splits the input feature X and output variable y into training, and testing set. Each of the classifier will be trained on the training set. To evaluate the model on unseen data, the score method of yellowbrick visualizers will be called using the testing set.\n",
    "\n",
    "## Training the Model\n",
    "The code below will train the 5 machine learning models using the preprocessed dataset from [Data Wrangling](exploring-data.qmd) section.\n",
    "\n",
    "## Classification Report\n",
    "The following series of plots below shows the ROC curve, confusion matrix, and classification report of each models.\n",
    "This visualization can be used to compare the performance of each of the model.\n",
    "\n",
    "\n",
    "\n",
    "While generating content, make sure that the following will be followed:\n",
    "1. Make it short, concise, but interesting.\n",
    "2. The tone should be professional, academic, technical, formal, and compelling.\n",
    "3. The copy should be cohesive and tells a compelling and interesting story.\n",
    "4. Return your results in markdown.\n",
    "5. Generate markdown headings when necessary.\n",
    "6. Generate the last paragraph concluding all the things that was made so far and what are the next step to be made.\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": instructions\n",
    "    },\n",
    "]\n",
    "\n",
    "results = get_completion_from_messages(messages=messages)\n",
    "Markdown(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LearnML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
