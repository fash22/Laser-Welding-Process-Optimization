# Modelling
On the [Data Wrangling](exploring-data.qmd) section, it was found that the 6-machine paramaters to be studied were discrete values with three levels each, and the target variable was a highly imbalanced binary variable.

In this section, the dataset will be fed into different machine learning models to find an optimal model that can help us understand the mapping between the input and output variables. To do this, the dataset should be splitted into two parts namely, the training set, and the testing set with test size of 20%. The training set will be used during model training, and cross validation while the testing set will be used to evaluate the model performance on unseen data.

Once the data was splitted, it will be loaded to different out-of-the-box machine learning models from scikit-learn library.
At this stage, the only goal is to find the optimal model using their default hyperparameters.

Finally, each of the model's training and testing performance will be compared and evaluated. The goal is to shortlist the top performing model.

## Importing Relevant Libraries
The following code below contains all of the import statements for this section.

```{python}
# Data Wrangling
import numpy as np
import pandas as pd

# Data Visualization
import seaborn as sns
import matplotlib.pyplot as plt

# Dataset Splitting
from sklearn.model_selection import train_test_split

# Feature Engineering Classes
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# Machine Learning Model Classes
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import (
    RandomForestClassifier,
    VotingClassifier,
    BaggingClassifier,
    AdaBoostClassifier,
    StackingClassifier
)

# Classification Visualization
from yellowbrick.classifier import ConfusionMatrix, ClassificationReport, ROCAUC

# Base Classes
from sklearn.base import ClassifierMixin, TransformerMixin
```


## Importing the Dataset
The following code below loads the dataset pre-processed from [EDA](exploring-data.qmd) section into pandas `DataFrame`.
```{python}
dataset_uri = "datasets/V1.csv"
dataset = pd.read_csv(dataset_uri, index_col=0)

dataset.head()
```

## Vertical Data Splitting
Once loaded, the dataset will be splitted into feature matrix `X`, and target vector `y`.
```{python}
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values
```

## Horizontal Data Splitting
Once the feature matrix and target vector was defined, they will be splitted horizontally into training set and testing set.
The code below will perform the horizontal split with test size of 20% with ensuring that both sets contains the same proportion of `0`s and `1`s on their target variable.

## Modelling
To compare different models, the yellowbrick datavisualization library was used. This library contains out-of-the-box data visualization tools for comparing different machine learning algorithms implemented in scikit-learn.
The class definition below provides an abstraction in comparing different machine learning models.
Its constructor accepts a list of estimators, an input feature X, and output variable y. It has a show_classification_report method that returns a data visualization of each of the model's ROC curve, confusion matrix, and classification report.
```{python}
class ClassifierEvaluator():
    def __init__(self, estimators: list[ClassifierMixin], X, y) -> None:
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            X,
            y,
            test_size=0.2,
            stratify=y,
            random_state=22
        )
        self.estimators = estimators

    def show_classification_reports(self) -> None:
        for estimator in self.estimators:
            name = type(estimator).__name__

            pipeline = Pipeline(
                steps=[
                    ("std_scaler", StandardScaler()),
                    ("classifier", estimator)
                ]
            )

            pipeline.target_type_ = 'binary'

            # Plot The Report
            fig, axes = plt.subplots(1,3)
            fig.set_size_inches(12,6)

            roc = ROCAUC(pipeline, ax=axes[0], binary=True)
            cm = ConfusionMatrix(pipeline, ax=axes[1], classes=["No Crack", "Crack"])
            cr = ClassificationReport(pipeline, support=True, ax=axes[2], classes=["Crack", "No Crack"])

            # Fit the model and the visualizers on the training data
            roc.fit(self.X_train, self.y_train)
            cm.fit (self.X_train, self.y_train)
            cr.fit (self.X_train, self.y_train)

            # Score and show the visualizers on the test data
            roc.score(self.X_test, self.y_test);
            cm.score (self.X_test, self.y_test);
            cr.score (self.X_test, self.y_test);

            fig.suptitle(f"{type(estimator).__name__}")
            axes[0].set_title(f"{type(roc).__name__}")
            axes[1].set_title(f"{type(cm).__name__}")
            axes[2].set_title(f"{type(cr).__name__}")

            axes[1].set_xlabel("Prediction")
            axes[1].set_ylabel("Actual")
            axes[2].set_xlabel("Prediction")
            axes[2].set_ylabel("Actual")
            
            axes[0].legend();

            plt.tight_layout()
            plt.show()
```

## Training the Model
The code below will train the 5 machine learning models using the preprocessed dataset from [Data Wrangling](exploring-data.qmd) section.
```{python}
evaluator = ClassifierEvaluator(
    estimators=[
        LogisticRegression(),
        DecisionTreeClassifier(),
        SVC(),
        GaussianNB(),
        KNeighborsClassifier()
    ],
    X=X,
    y=y
)
```

## Classification Report
The following series of plots below shows the ROC curve, confusion matrix, and classification report of each models.
This visualization can be used to compare the performance of each of the model.
```{python}
# | column: page
# | layout-nrow: 5
# | fig-cap:
# |     - "**Logistic Regression Model**: *Precision (95.3%)* - Out of 64 samples that the model predicted to have no cracks on welding joints, 3 of them actually have a crack. *Recall (70.00%)* - Out of all 10 samples with cracks on welding joint, the model was able to correctly predict 7 of them."
# |     - "**Decision Tree Model**: *Precision (95.3%)* - Out of 64 samples that the model predicted to have no cracks on welding joints, 3 of them actually have a crack. *Recall (70.00%)* - Out of all 10 samples with cracks on welding joint, the model was able to correctly predict 7 of them. Although its recall, and precsion was same with Logistic regression model, since the area-under-curve of its receiver operating curve was lower thank logistic regression model, it has lesser discriminability. "
# |     - "**Support Vector Model**: *Precision (96.8%)* - Out of 62 samples that the model predicted to have no cracks on welding joints, 2 of them actually have a crack. *Recall (80.00%)* - Out of all 10 samples with cracks on welding joint, the model was able to correctly predict 8 of them. At 0.97 AUC, this model's discriminability is better than the decision tree model."
# |     - "**Gaussian Naive Baye's**: *Precision (100.00%)* - Out of 47 samples that the model predicted to have no cracks on welding joints, 0 of them actually have a crack. *Recall (100.00%)* - Out of all 10 samples with cracks on welding joint, the model was able to correctly predict 10 of them."
# |     - "**K-Nearest-Neighbors Model**: *Precision (92.2%)* - Out of 64 samples that the model predicted to have no cracks on welding joints, 5 of them actually have a crack. *Recall (50%)* - Out of all 10 samples with cracks on welding joint, the model was able to correctly predict 5 of them. This is the worse model since its ability to correctly predict welding cracks was almost like tossing a fair coin."

evaluator.show_classification_reports()
```

## Model Evaluation
Now, let's analyze each of the model according to their recall rate in decreasing order while considering their impact on cost.

### GaussianNB Model
If a manufacturing business unit does not tolerate any defect out-flow to the next process, a conservative models very high recall rate could be employed such as the GaussianNB model with 100.00% recall-rate (10/10). However keep in mind that this has trade-off -- a high false-positive rate of 24.20% (15/62). If the cost of rework due to false detection of welding cracks outweighs the cost of re-inspection and re-processing due to defect outflow, then this model is not feasible.

### Support Vector Classifier
If the business unit allows a compromise (a small rate of defect outflow), while keeping the false-rejection rate at minimum, then the Support Vector Classifier model could be used. Although its recall-rate of 80.00% (8/10) is much smaller than the GaussianNB model, its feasibility can be justified by its low false positive rate of 3.23% (2/62). If the cost of rework due to false detection of welding cracks outweighs the cost of reinspection and reprocessing due to defect outflow, this model is more desirable than GuassianNB.

### Logistic Regression and Decision Tree
These models have low recall rate of 70.00% (7/10), and precision rate of 95.30% (61/64), However, their false positive rate of 1.61% (1/62) were superior against GaussianNB and SVC. Even though these models have superior false positive rate, using them however, bears a significant risk of defect out-flow which might result to poor customer satisfaction.

### KNN Classifier
Out of all of the 5-models, this is the worse performing one with inferior recall rate of 50.00% (5/10), inferior precision rate of 92.2% (59/64), and inferior false positive rate of 4.84% (3/62)

### Conclusion
Based on the results of the model evaluation, the most feasible model to be used in terms of balance between false positive rate and recall rate was the support vector classifier. Another notable model with good detection of cracks was the Gaussian Naive Baye's model, however some work should be done to reduce its false positive rate. On the next phase of this project, we will focus our efforts improving the performance of these two models through feature engineering and hyper-parameter tuning. The last three poor classifiers will be revisited in the Ensembling phase to check if they can help in improving the detection of welding cracks.