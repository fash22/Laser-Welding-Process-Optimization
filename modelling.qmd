# Machine Learning Model Comparison
In the previous section on [Data Wrangling](exploring-data.qmd), it was discovered that the 6 machine parameters to be studied had discrete values with three levels each, and the target variable was a highly imbalanced binary variable.

In this section, we will feed the dataset into different machine learning models to find the optimal model that can help us understand the relationship between the input and output variables. To do this, we will split the dataset into a training set and a testing set, with a test size of 20%. The training set will be used for model training and cross-validation, while the testing set will be used to evaluate the model's performance on unseen data.

Once the data is split, we will load it into different out-of-the-box machine learning models from the scikit-learn library. At this stage, our goal is to find the optimal model using their default hyperparameters.

Finally, we will compare and evaluate the training and testing performance of each model. The objective is to shortlist the top-performing model.

## Importing Relevant Libraries
The following code contains all the necessary import statements for this section.

```{python}
# Data Wrangling
import numpy as np
import pandas as pd

# Data Visualization
import seaborn as sns
import matplotlib.pyplot as plt

# Dataset Splitting
from sklearn.model_selection import train_test_split

# Feature Engineering Classes
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# Machine Learning Model Classes
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import (
    RandomForestClassifier,
    VotingClassifier,
    BaggingClassifier,
    AdaBoostClassifier,
    StackingClassifier
)

# Classification Visualization
from yellowbrick.classifier import ConfusionMatrix, ClassificationReport, ROCAUC

# Base Classes
from sklearn.base import ClassifierMixin, TransformerMixin
```


## Importing the Dataset
The code below loads the pre-processed dataset from the EDA section into a pandas DataFrame.
```{python}
dataset_uri = "datasets/V1.csv"
dataset = pd.read_csv(dataset_uri, index_col=0)

dataset.head()
```

## Vertical Data Splitting
Once the dataset is loaded, we will split it into a feature matrix `X` and a target vector `y`.
```{python}
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values


```

## Horizontal Data Splitting
After defining the feature matrix and target vector, we will split them horizontally into a training set and a testing set. The code below performs the horizontal split with a test size of 20%, ensuring that both sets contain the same proportion of `0`s and `1`s in their target variable.

## Modelling
To compare different models, we will use the `yellowbrick` data visualization library. This library provides out-of-the-box data visualization tools for comparing different machine learning algorithms implemented in **scikit-learn**.

The class definition below provides an abstraction for comparing different machine learning models. Its constructor accepts a list of estimators, an input feature `X`, and an output variable `y`. It has a `show_classification_report` method that returns a data visualization of each model's *ROC curve*, *confusion matrix*, and *classification report*.

To generate these reports, the `ClassifierEvaluator` class internally splits the input feature `X` and output variable `y` into training and testing sets. Each classifier will be trained on the training set. To evaluate the model on unseen data, the score method of yellowbrick visualizers will be called using the testing set.

```{python}
class ClassifierEvaluator():
    def __init__(self, estimators: list[ClassifierMixin], X, y) -> None:
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            X,
            y,
            test_size=0.2,
            stratify=y,
            random_state=22
        )
        self.estimators = estimators

    def show_classification_reports(self) -> None:
        for estimator in self.estimators:
            name = type(estimator).__name__

            pipeline = Pipeline(
                steps=[
                    ("std_scaler", StandardScaler()),
                    ("classifier", estimator)
                ]
            )

            pipeline.target_type_ = 'binary'

            # Plot The Report
            fig, axes = plt.subplots(1,3)
            fig.set_size_inches(12,6)

            roc = ROCAUC(pipeline, ax=axes[0], binary=True)
            cm = ConfusionMatrix(pipeline, ax=axes[1], classes=["No Crack", "Crack"])
            cr = ClassificationReport(pipeline, support=True, ax=axes[2], classes=["Crack", "No Crack"])

            # Fit the model and the visualizers on the training data
            roc.fit(self.X_train, self.y_train)
            cm.fit (self.X_train, self.y_train)
            cr.fit (self.X_train, self.y_train)

            # Score and show the visualizers on the test data
            roc.score(self.X_test, self.y_test);
            cm.score (self.X_test, self.y_test);
            cr.score (self.X_test, self.y_test);

            fig.suptitle(f"{type(estimator).__name__}")
            axes[0].set_title(f"{type(roc).__name__}")
            axes[1].set_title(f"{type(cm).__name__}")
            axes[2].set_title(f"{type(cr).__name__}")

            axes[1].set_xlabel("Prediction")
            axes[1].set_ylabel("Actual")
            axes[2].set_xlabel("Prediction")
            axes[2].set_ylabel("Actual")
            
            axes[0].legend();

            plt.tight_layout()
            plt.show()
```

## Training the Model
The code below will train 5 machine learning models using the preprocessed dataset from the Data Wrangling section.
```{python}
evaluator = ClassifierEvaluator(
    estimators=[
        LogisticRegression(),
        DecisionTreeClassifier(),
        SVC(),
        GaussianNB(),
        KNeighborsClassifier()
    ],
    X=X,
    y=y
)
```

## Classification Report
The following series of plots show the ROC curve, confusion matrix, and classification report of each model. This visualization can be used to compare the performance of each model.

```{python}
# | column: page
# | layout-nrow: 5
# | fig-cap:
# |     - "The logistic regression model achieved a precision of 95.3%, correctly predicting 61 out of 64 samples that were predicted to have no cracks on welding joints. However, it had a recall of 70.00%, meaning that it only correctly predicted 7 out of 10 samples with cracks."
# |     - "Similar to the logistic regression model, the decision tree model also achieved a precision of 95.3% and a recall of 70.00%. However, its area-under-curve (AUC) of the receiver operating curve was lower than the logistic regression model, indicating lower discriminability."
# |     - "The support vector model outperformed both the logistic regression and decision tree models with a precision of 96.8% and a recall of 80.00%. It correctly predicted 62 out of 64 samples without cracks and 8 out of 10 samples with cracks. Additionally, it had an AUC of 0.97, indicating better discriminability than the decision tree model."
# |     - "The Gaussian Naive Bayes model achieved perfect precision and recall, correctly predicting all 47 samples without cracks and all 10 samples with cracks. This indicates that it is a highly accurate model for predicting welding cracks."
# |     - "The K-Nearest Neighbors model had a precision of 92.2%, correctly predicting 59 out of 64 samples without cracks. However, it had a low recall of 50%, only correctly predicting 5 out of 10 samples with cracks. This indicates that the model's ability to predict welding cracks is not much better than random chance."

evaluator.show_classification_reports()
```

## Evaluating Models for Detecting Welding Cracks
In this analysis, we evaluate different models based on their recall rate and their impact on cost. We aim to find the most feasible model that balances the trade-off between false positive rate and recall rate.

### GaussianNB Model
For manufacturing businesses that cannot tolerate any defect outflow, the GaussianNB model with a recall rate of 100.00% (10/10) can be employed. However, it comes with a high false positive rate of 24.20% (15/62). The feasibility of this model depends on whether the cost of rework due to false detection outweighs the cost of re-inspection and re-processing.

### Support Vector Classifier
The Support Vector Classifier model offers a compromise, allowing a small rate of defect outflow while minimizing the false rejection rate. It has a recall rate of 80.00% (8/10) and a low false positive rate of 3.23% (2/62). If the cost of rework due to false detection is lower than the cost of re-inspection and re-processing, this model is a desirable choice.

### Logistic Regression and Decision Tree
Both Logistic Regression and Decision Tree models have a recall rate of 70.00% (7/10) and a precision rate of 95.30% (61/64). They also have a superior false positive rate of 1.61% (1/62) compared to GaussianNB and SVC. However, using these models carries a significant risk of defect outflow, which can result in poor customer satisfaction.

### KNN Classifier
The KNN Classifier performs the worst among the five models, with a recall rate of 50.00% (5/10), a precision rate of 92.2% (59/64), and a false positive rate of 4.84% (3/62).

### Conclusion
Based on our evaluation, the most feasible model that balances the false positive rate and recall rate is the Support Vector Classifier. The GaussianNB model also shows promise in detecting cracks, but its false positive rate needs improvement. In the next phase of this project, we will focus on improving the performance of these two models through feature engineering and hyper-parameter tuning. The three poor classifiers will be revisited in the Ensembling phase to see if they can contribute to improving the detection of welding cracks.